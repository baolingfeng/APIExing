At the first step I used ` df.T ` to transpose the dataframe , and tried something like ` df.value_counts() ` , however I'd
Replace NaN in a dataframe with random values
I want to replace all the NaN with some random values like . #CODE
Resample function throwing error with Twitter Data
I then try to resample for analysis #CODE
I'm writing several pivot tables using pandas . For many of them , I need to return unique values . In a two-dimensional pivot table , the below code works as it should . When I add a third dimension , the code returns the count rather than the unique count . I suspect this has something to do with the aggfunc , but can't determine to what it should be changed .
Use a groupby to get at each combination of ` col_1 ` and ` col_3 ` , then unstack to get the ` col_3 ` values as columns : #CODE
Python pandas merge or concat dataframes
The data is for 2 products ( BBG.XAMS.UL.S_pnl_pos_cost and BBG.XAMS.UNA.S_pnl_pos_cost ) by date , in the future there will be more products . I want to concat or merge ( not sure which ) the list of dataframes into one data frame ( called result ) so they look like : #CODE
where axis is the date . It looks like the data is merged by date , but I am missing the data for the week beginning 2015-03-23 . My current concat result dataframe looks like : #CODE
Try using axis=0 . This should concat column-wise , assuming each dataframe has the same column names .
possible duplicate of [ Pandas join / merge / concat two dataframes ] ( #URL )
Also , how do you join this back to original dataframe ?
We can resample this to days ; it'll be a much longer timeseries , of course , but memory is cheap and I'm lazy : #CODE
How do I merge the birth rate back to the original table ? Indexes aren't compatible ... Turns out size isn't such an issue .
Python & Pandas : Unable to drop columns
I try to drop the data , but it reports some column does not exist . #CODE
@USER , that's possible , but I don't know how to deal with it . In my previous experience with pandas , it will automatically turn the second ` Q ` into ` Q.1 ` when reading the data . However , in my case , it failed to do it , and I don't know why . However , This it cannot ` drop ` ` NCDC ` either .
You can then drop your columns : #CODE
I would like to take a given row from a DataFrame and prepend or append to the same DataFrame .
Rather than concat I would just assign directly to the df after ` shift ` ing , then use ` iloc ` to reference the position you want to assign the row , you have to call ` squeeze ` so that you assign just the values and lose the original index value otherwise it'll raise a ` ValueError ` : #CODE
To insert at the end : #CODE
I'm not sure exactly what you're expecting , but you could replace your lists with numpy arrays ( I don't think it'll improve your specific code ): #CODE
What is the best way for me to get this data into Pandas ? Is there a standard way I could use ` read_table ` or some similar function to read this file directly ? Should I write a script to insert commas where all the column breaks are and then read it in as CSV ? ( I'd just do the latter , but I'm also interested in becoming better with Pandas so if there's an out-of-the-box way I'd like to know it . )
Any ideas on how to get this file to load ? Unfortunately I can't just strip out accents , as I have to interface with software that requires the proper name , and I have a ton of files to format ( not just the one ) . Thanks !
pls show your input and what is the expected output , in a copy-pastable form . What you are doing is very inefficient . A groupby should try to use vectorized functions when possible . Then join them up at the end .
Hi Tom , it doesn't look like this works . It outputs just one array and is equivalent to df2 [ ' array '] .sum() . But you have given me an idea with apply . Let me see if I can figure something out .
You need ` apply ( your_func , axis=1 )` to work on a row-by-row basis . #CODE
Another way would be to call ` unique ` on the transpose of your df : #CODE
Drop values satisfying condition plus arbitrary number of next values in a pandas DataFrame
So my final goal is to drop values in one column of a ` pandas ` ` DataFrame ` according to some condition on another column of the same ` DataFrame ` , plus several next values e.g. : #CODE
So this will drop the records where the condition is satisfied , but how do I drop the next 3 records after the condition was satisfied too ? My desired output would look something like this : #CODE
We can use the boolean condition index to slice the df using ` loc ` and set the following values : #CODE
Panda's boxplot but not showing the box
Note , ` showbox ` and ` whiskerprops ` are the ` kwds ` of boxplot , which are in turn passed to ` matplotlib.boxplot ` .
Applying aggregate function on columns of Pandas pivot table
I generated the following pivot table via taking maximum of values in ` Z ` column : #CODE
Here's a fairly general solution you can apply to multiple columns . The ' To ' column doesn't need to be rounded , I just included it for the generality of two columns rather than one : #CODE
428 base , mult = _gfc ( freq )
--> 429 return tslib.dt64arr_to_periodarr ( data.view ( ' i8 ') , base , tz )
I could do a left merge , but I would end up with a huge file . Is there any way to add specific rows from df2 to df1 using merge ?
Unclear why you think a left merge would produce a huge file , by performing a left merge on the product id you are stating that you are only interested in matches in the product_id column only
Just perform a left ` merge ` on ' product_id ' column : #CODE
What would be the Python equivalent ? I cannot think of a way to translate this where statement into pandas syntax .
The only way I can think of is to add an arbitrary field to people_usa ( e.g. ` people_usa [ ' dummy '] =1 `) , do a left join , then take only the records where ' dummy ' is nan , then delete the dummy field - which seems a bit convoluted .
Does this work only on the index of the dataframe ? I'd like the option to specify the field ( s ) to apply this to
Is there any easy way to do this if you have multiple columns to check / join ?
You could do a ` merge ` and then eliminate the rows that exist in the merged df otherwise you'd have to build a boolean condition for all the columns you want to compare but presumably when checking the multiple columns you're stating that it's unique for those columns , correct ? For instance it's not a match if say col1 and col2 match but col3 does not
Yes merge is what I have been doing but it feels like a hassle .
I've come up with this , using itertools , to find mid-day timestamps and group them by date , and now I'm coming up short trying to apply imap to find the means . #CODE
Since not sure what your end output should look like , just create a time-based grouper manually ( this is essentially a resample ) , but doesn't do anything with the final results ( its just a list of the aggregated values ) #CODE
You can get reasonable fancy here and say return a pandas object ( and potentially ` concat ` them ) .
and I want to pivot it like this : #CODE
I am calling a function from within a ' for each loop ' which attempts to insert values into a Pandas DataFrame based on a specified column start and end location . The function is this : #CODE
My issue is that despite the same starting conditions when I call this function it seems to generate a list of inconsistent length . e.g. with values of srowb = 1 and erowb = 18 it will generate a list ( tmp_brollb ) which has either len ( tmp_brollb ) = 17 or len ( tmp_brollb ) = 18
Use ` max ` and check for equality using ` eq ` and cast the boolean df to int using ` astype ` , this will convert ` True ` and ` False ` to ` 1 ` and ` 0 ` : #CODE
Thanks @USER . Did you try my original post ? I would be interested to know how much time this one is taking compared to yours ? ` for i in range ( len ( df )): ... df.loc [ i ] [ df.loc [ i ] .idxmax ( axis=1 )] = 1 ... df.loc [ i ] [ df.loc [ i ] ! = 1 ] = 0 `
I am trying to normalize the missing values in matrix . Here is the code . #CODE
Last line should replace the values in dataset1 by mean values from ` ds2_mean [ 1 ]` . But it does not do . Anything wrong here ?
And after that can I replace NaN with the average value of it's neighbours in dataset1 ?
it does wrong . For any x in dataset2 it has mapped value in col2 . It should replace all values of x in ds1 by mapped value . But this also does not do it
Sorry can you explain clearer , what are you mapping from what to what exactly ? By default fillna will use the index so how do you want the mapping from ` ds2 ` to map to the missing values in ` ds1 ` ? Are you wanting to map using the values in ` ds2 [ 0 ]` as the index lookup ? So use the index from ` ds1 ` find value in ` ds2 [ 0 ]` and return ` ds2 [ 1 ]` ?
yes , I want to use the index from ds1 find value in ds2 [ 0 ] and replace it with ds2 [ 1 ]" sorry for inconvenience
I want to add a new column which contains values based on df [ ' diff ']
When using ` DataFrame.apply ` if you use ` axis=0 ` it applies the condition through columns , to use ` apply ` to go through each row , you need ` axis=1 ` .
But given that , you can use ` Series.apply ` instead of ` DataFrame.apply ` on the `' diff '` series . Example - #CODE
You can just set all the values that meet your criteria rather than looping over the df by calling ` apply ` so the following should work and as it's vectorised will scale better for larger datasets : #CODE
this will set all rows that meet the criteria , the problem using ` apply ` is that it's just syntactic sugar for a ` for ` loop and where possible this should be avoided where a vectorised solution exists .
Then you can ` stack ` ( first by `' Marker '` then by `' mrk '`) : #CODE
Python DataFrame - apply different calculations due to a column's value
You could do this using 2 ` loc ` calls : #CODE
There are two reasons whiskers length vary from one boxplot to any other boxplot
Are you asking why the top whisker isn't the same length as the bottom ? I think the whiskers are actually the lowest or highest data point within 1.5 IQR . So if there are no data points between Q3 and Q3 + 1.5 IQR , then the top whisker won't show up . For the one boxplot where the are outliers beyond the whiskers on both the top and the bottom , the whiskers do look about the same size .
`` hist `` -> `` histogram `` ( `` hist `` is pyplot or something ) .
There is a pandas equivalent to this ` cut ` there is a section describing this here . ` cut ` returns the open closed intervals for each value : #CODE
Pandas Dataframe , Apply Function , Return Index
Then I can apply the function to my dataframe , grouped by I D: #CODE
If I resample this DataField by any frequency , the timezone is kept : #CODE
their are a couple of outstanding bugs w.r.t to resample and extra binning : #URL if you would like to investigate and try to pinpoint ( or better yet fix ) would be appreciated ! you can comment on that issue directly
@USER ; You mean to the stack exchange answer ?
